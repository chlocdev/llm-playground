{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a8e67e-b806-4782-bfed-0cbfb391c13c",
   "metadata": {},
   "source": [
    "Criterias:\n",
    "\n",
    "- Name\n",
    "- Languages\n",
    "- Licence\n",
    "- Tasks\n",
    "- Context Length\n",
    "- GPU\n",
    "- Code\n",
    "\n",
    "https://huggingface.co/deepseek-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43bb0634-462a-4be9-82a1-8fe52eeed81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459bfc9b-c4b0-46a8-a515-497f396a5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "\n",
      "positional arguments:\n",
      "  {download,upload,repo-files,env,login,whoami,logout,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}\n",
      "                        huggingface-cli command helpers\n",
      "    download            Download files from the Hub\n",
      "    upload              Upload a file or a folder to a repo on the Hub\n",
      "    repo-files          Manage files in a repo on the Hub\n",
      "    env                 Print information about the environment.\n",
      "    login               Log in using a token from\n",
      "                        huggingface.co/settings/tokens\n",
      "    whoami              Find out which huggingface.co account you are logged\n",
      "                        in as.\n",
      "    logout              Log out\n",
      "    repo                {create} Commands to interact with your huggingface.co\n",
      "                        repos.\n",
      "    lfs-enable-largefiles\n",
      "                        Configure your repository to enable upload of files >\n",
      "                        5GB.\n",
      "    scan-cache          Scan cache directory.\n",
      "    delete-cache        Delete revisions from the cache directory.\n",
      "    tag                 (create, list, delete) tags for a repo in the hub\n",
      "    version             Print information about the huggingface-cli version.\n",
      "    upload-large-folder\n",
      "                        Upload a large folder to a repo on the Hub\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5dade32-6773-4275-846b-92309ec2988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO ID                                                    REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED  LAST_MODIFIED  REFS LOCAL PATH                                                                                           \n",
      "---------------------------------------------------------- --------- ------------ -------- -------------- -------------- ---- ---------------------------------------------------------------------------------------------------- \n",
      "glue                                                       dataset           1.1M        4 1 hour ago     1 hour ago          /home/loc/.cache/huggingface/hub/datasets--glue                                                      \n",
      "CohereForAI/aya-23-8B                                      model            16.1G       14 3 hours ago    3 hours ago    main /home/loc/.cache/huggingface/hub/models--CohereForAI--aya-23-8B                                      \n",
      "Fsoft-AIC/XMAiNframe-instruct-7b                           model            13.8G       17 22 minutes ago 22 minutes ago main /home/loc/.cache/huggingface/hub/models--Fsoft-AIC--XMAiNframe-instruct-7b                           \n",
      "OrionStarAI/Orion-14B-Chat                                 model            58.0G       31 2 hours ago    60 minutes ago main /home/loc/.cache/huggingface/hub/models--OrionStarAI--Orion-14B-Chat                                 \n",
      "Qwen/Qwen2.5-7B-Instruct                                   model            15.2G       14 2 hours ago    2 hours ago    main /home/loc/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct                                   \n",
      "SnypzZz/Llama2-13b-Language-translate                      model             7.3G       10 2 hours ago    2 hours ago    main /home/loc/.cache/huggingface/hub/models--SnypzZz--Llama2-13b-Language-translate                      \n",
      "bert-base-cased                                            model           436.4M        5 4 hours ago    4 hours ago    main /home/loc/.cache/huggingface/hub/models--bert-base-cased                                             \n",
      "bert-base-uncased                                          model           441.1M        5 2 hours ago    2 hours ago    main /home/loc/.cache/huggingface/hub/models--bert-base-uncased                                           \n",
      "distilbert/distilbert-base-uncased-finetuned-sst-2-english model           268.1M        4 4 hours ago    4 hours ago    main /home/loc/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english \n",
      "distilbert-base-uncased-finetuned-sst-2-english            model           268.1M        4 4 hours ago    4 hours ago    main /home/loc/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english             \n",
      "ehristoforu/Gistral-16B                                    model            33.7G       14 54 minutes ago 39 minutes ago main /home/loc/.cache/huggingface/hub/models--ehristoforu--Gistral-16B                                    \n",
      "meta-llama/Meta-Llama-3-8B-Instruct                        model            32.1G       17 21 minutes ago 8 minutes ago  main /home/loc/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct                        \n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct                      model            32.1G       17 38 minutes ago 22 minutes ago main /home/loc/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct                      \n",
      "mistralai/Mistral-Nemo-Instruct-2407                       model            49.0G       18 2 hours ago    2 hours ago    main /home/loc/.cache/huggingface/hub/models--mistralai--Mistral-Nemo-Instruct-2407                       \n",
      "\n",
      "Done in 0.0s. Scanned 14 repo(s) for a total of \u001b[1m\u001b[31m258.9G\u001b[0m.\n",
      "\u001b[90mGot 1 warning(s) while scanning. Use -vvv to print details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f200b29f-41df-4e3a-88a3-02ff502ab30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locchuong\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d8e55-a304-407b-985e-6e3e1bfc9a69",
   "metadata": {},
   "source": [
    "### Fsoft-AIC/XMAiNframe-instruct-7b\n",
    "\n",
    "https://huggingface.co/Fsoft-AIC/XMAiNframe-instruct-7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56fa98a9-ad88-4633-bd2f-997acc8d06da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7734959f7f4445a3b63a7ef57eba984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--Fsoft-AIC--XMAiNframe-instruct-7b/snapshots/15213059af78dc5d46c71c836b0ee9df5bbed654\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=\"Fsoft-AIC/XMAiNframe-instruct-7b\", \n",
    "                              cache_dir=\"/home/loc/.cache/huggingface/hub\")  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512b3a95-f25a-4145-81f9-16065337a61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c4f46b998f4ea0851107a93e1c9682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Mainframe is uncertain, as many organizations are gradually shifting their focus towards cloud computing and distributed systems. However, Mainframes will continue to play a vital role in critical applications and industries that require high levels of security, reliability, and performance.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Fsoft-AIC/XMAiNframe-instruct-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Fsoft-AIC/XMAiNframe-instruct-7b\",\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             device_map=\"auto\",\n",
    "                                             # offload_folder=\"offload\",  # Folder to offload weights to disk\n",
    "                                             # offload_state_dict=True    # Also offload the state_dict to disk\n",
    "                                            )\n",
    "messages=[\n",
    "    {'from':'system', 'value': \"You are a helpful assistant\"},\n",
    "    {'from': 'human', 'value': 'What is the future of Mainframe?'}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    " \n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738d1bf-7ba1-432f-8d71-0a57b6fa2b5a",
   "metadata": {},
   "source": [
    "### CohereForAI/aya-23-8B\n",
    "\n",
    "https://huggingface.co/CohereForAI/aya-23-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe6169c-638a-4686-94e2-b199f878209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded at: ./my_cache/models--CohereForAI--aya-23-8B/snapshots/ce57174a5f93ee0e7edaf5020465bf2c0a824381/README.md\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download a file from a model repository\n",
    "file_path = hf_hub_download(repo_id=\"CohereForAI/aya-23-8B\", \n",
    "                            filename=\"README.md\", \n",
    "                            cache_dir=\"./my_cache\")  # optional cache directory\n",
    "\n",
    "print(f\"File downloaded at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962765c2-05ad-4bde-bb91-3d1902b9d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693215f3571d4de397cd8b2578d1488f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--CohereForAI--aya-23-8B/snapshots/ce57174a5f93ee0e7edaf5020465bf2c0a824381\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=\"CohereForAI/aya-23-8B\", \n",
    "                              cache_dir=\"/home/loc/.cache/huggingface/hub\")  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cbb8b-a135-41eb-a029-7d62c63a97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers==4.41.1\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"CohereForAI/aya-23-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            device_map=\"auto\",)\n",
    "\n",
    "# Format message with the command-r-plus chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=100, \n",
    "    do_sample=True, \n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0])\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c448c-0854-4685-a8ff-df622aebcc81",
   "metadata": {},
   "source": [
    "### Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e22b14-ea81-49ed-92b0-7bcc74997fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5290be35c7e43eaaf585ee1dad6c5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/acbd96531cda22292a3ceaa67e984955d3965282\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e28894-3389-4097-ace6-ca2d03300264",
   "metadata": {},
   "source": [
    "### mistralai/Mistral-Nemo-Instruct-2407\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9526a1-f84b-42b6-80c0-871fc158c2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7ba2d783d746149fbf83fedbb026d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--mistralai--Mistral-Nemo-Instruct-2407/snapshots/e17a136e1dcba9c63ad771f2c85c1c312c563e6b\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f711d-1ff2-46d8-a54f-5f830d7bcb61",
   "metadata": {},
   "source": [
    "### SnypzZz/Llama2-13b-Language-translate\n",
    "\n",
    "https://huggingface.co/SnypzZz/Llama2-13b-Language-translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06394f3-2132-4d2e-bd46-c9835c3d95d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420f602b0d6b4faaa313a222f26ea89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--SnypzZz--Llama2-13b-Language-translate/snapshots/ea2e28ed30d327b41ae4056ff909cdf3bb7d56b1\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"SnypzZz/Llama2-13b-Language-translate\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fda9a-7fdb-4c7f-9120-44c473939351",
   "metadata": {},
   "source": [
    "### OrionStarAI/Orion-14B-Chat\n",
    "\n",
    "https://huggingface.co/OrionStarAI/Orion-14B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed6a409-ce2e-4c69-a289-d1a8a9a8ba9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42bd38c7ff74ad39d021adde25ede8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 31 files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--OrionStarAI--Orion-14B-Chat/snapshots/7aa75f1e0939fc082e67a7f58af7876907a1875e\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"OrionStarAI/Orion-14B-Chat\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd26f98-3afc-4618-a6e1-2cf50727ec09",
   "metadata": {},
   "source": [
    "### ehristoforu/Gistral-16B\n",
    "\n",
    "https://huggingface.co/ehristoforu/Gistral-16B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93f8833-7c0d-437f-baa1-e6b81aa60b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa980e3517804d65b1d8512abd9cad3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--ehristoforu--Gistral-16B/snapshots/978beecf7e54cbd2511a69d8cd16cec87f1200de\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"ehristoforu/Gistral-16B\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd45d0-ef17-4acc-b593-9df5c39ed4f0",
   "metadata": {},
   "source": [
    "### meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb690983-ac5c-4ff3-a137-e0ffa8f80df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d40263733b494eb84481ef7ad96305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df1b49-adbf-47a7-a0a8-c7ae6380f610",
   "metadata": {},
   "source": [
    "### meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "397f7111-459f-4b93-8cec-ee7845a1d8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f731d495a841a99afc99392bfd2dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded to: /home/loc/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "cache_dir = \"/home/loc/.cache/huggingface/hub\"\n",
    "\n",
    "# Download all files from the model repository\n",
    "local_dir = snapshot_download(repo_id=repo_id, \n",
    "                              cache_dir=cache_dir)  # optional cache directory\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f6b76-2dad-4212-bacb-583b0a2f5d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
