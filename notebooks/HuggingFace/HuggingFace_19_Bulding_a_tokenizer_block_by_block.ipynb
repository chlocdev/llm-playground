{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31da264c-3b83-4434-87f2-e14b80ed173d",
   "metadata": {},
   "source": [
    "###  Building a tokenizer, block by block\n",
    "\n",
    "As we’ve seen in the previous sections, tokenization comprises several steps:\n",
    "\n",
    "- Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n",
    "\n",
    "- Pre-tokenization (splitting the input into words)\n",
    "\n",
    "- Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n",
    "\n",
    "- Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)\n",
    "\n",
    "As a reminder, here’s another look at the overall process:\n",
    "\n",
    "![process](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316aca1f-8c16-41b0-ade3-9d335dee146a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
