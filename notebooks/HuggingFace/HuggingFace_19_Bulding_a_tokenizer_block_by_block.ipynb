{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31da264c-3b83-4434-87f2-e14b80ed173d",
   "metadata": {},
   "source": [
    "###  Building a tokenizer, block by block\n",
    "\n",
    "As weâ€™ve seen in the previous sections, tokenization comprises several steps:\n",
    "\n",
    "- Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n",
    "\n",
    "- Pre-tokenization (splitting the input into words)\n",
    "\n",
    "- Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n",
    "\n",
    "- Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)\n",
    "\n",
    "As a reminder, hereâ€™s another look at the overall process:\n",
    "\n",
    "![process](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg)\n",
    "\n",
    "The ðŸ¤— Tokenizers library has been built to provide several options for each of those steps, which you can mix and match together. In this section weâ€™ll see how we can build a tokenizer from scratch, as opposed to training a new tokenizer from an old one as we did in [section 2](https://huggingface.co/course/chapter6/2). Youâ€™ll then be able to build any kind of tokenizer you can think of!\n",
    "\n",
    "\n",
    "\n",
    "More precisely, the library is built around a central `Tokenizer` class with the building blocks regrouped in submodules:\n",
    "\n",
    "- `normalizers` contains all the possible types of `Normalizer` you can use (complete list [here](https://huggingface.co/docs/tokenizers/api/normalizers)).\n",
    "\n",
    "- `pre_tokenizers` contains all the possible types of `PreTokenizer` you can use (complete list [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)).\n",
    "\n",
    "- `models` contains the various types of Model you can use, like `BPE`, `WordPiece`, and `Unigram` (complete list [here](https://huggingface.co/docs/tokenizers/api/models)).\n",
    "\n",
    "- `trainers` contains all the different types of `Trainer` you can use to train your model on a corpus (one per type of model; complete list [here](https://huggingface.co/docs/tokenizers/api/trainers)).\n",
    "\n",
    "- `post_processors` contains the various types of `PostProcessor` you can use (complete list [here](https://huggingface.co/docs/tokenizers/api/post-processors)).\n",
    "\n",
    "- `decoders` contains the various types of `Decoder` you can use to decode the outputs of tokenization (complete list [here](https://huggingface.co/docs/tokenizers/components#decoders)).\n",
    "\n",
    "You can find the whole list of building blocks [here](https://huggingface.co/docs/tokenizers/components)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd917f4-0de8-4df8-a7f5-788090173dfd",
   "metadata": {},
   "source": [
    "###  Acquiring a corpus\n",
    "\n",
    "To train our new tokenizer, we will use a small corpus of text (so the examples run fast). The steps for acquiring the corpus are similar to the ones we took at the [beginning of this chapter](https://huggingface.co/course/chapter6/2), but this time weâ€™ll use the [WikiText-2](https://huggingface.co/datasets/wikitext) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1129dfe-a763-42c4-a93c-bfea41651cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b2c9d2-d787-421f-abe8-0dc9b4e1d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i:i+1000][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141916d-8329-4f43-a13c-c832956e573d",
   "metadata": {},
   "source": [
    "The function `get_training_corpus()` is a generator that will yield batches of 1,000 texts, which we will use to train the tokenizer.\n",
    "\n",
    "ðŸ¤— Tokenizers can also be trained on text files directly. Hereâ€™s how we can generate a text file containing all the texts/inputs from WikiText-2 that we can use locally:\n",
    "\n",
    "```python\n",
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")\n",
    "```\n",
    "\n",
    "Next weâ€™ll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block. That will give us an example of each of the three main tokenization algorithms: WordPiece, BPE, and Unigram. Letâ€™s start with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75094d03-1ccb-4b9d-aaad-20b208591a42",
   "metadata": {},
   "source": [
    "###  Building a WordPiece tokenizer from scratch\n",
    "\n",
    "To build a tokenizer with the ðŸ¤— Tokenizers library, we start by instantiating a `Tokenizer` object with a `model`, then set its `normalizer`, `pre_tokenizer`, `post_processor`, and `decoder` attributes to the values we want.\n",
    "\n",
    "For this example, weâ€™ll create a `Tokenizer` with a `WordPiece` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00776320-53ce-4b74-a2f3-9aaa3996557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "decoders,\n",
    "models,\n",
    "normalizers,\n",
    "pre_tokenizers,\n",
    "processors,\n",
    "trainers,\n",
    "Tokenizer\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8d02d-2178-46ef-86a3-e598e349f1e7",
   "metadata": {},
   "source": [
    "We have to specify the `unk_token` so the model knows what to return when it encounters characters it hasnâ€™t seen before. Other arguments we can set here include the `vocab` of our model (weâ€™re going to train the model, so we donâ€™t need to set this) and `max_input_chars_per_word`, which specifies a maximum length for each word (words longer than the value passed will be split).\n",
    "\n",
    "The first step of tokenization is normalization, so letâ€™s begin with that. Since BERT is widely used, there is a `BertNormalizer` with the classic options we can set for BERT: `lowercase` and `strip_accents`, which are self-explanatory; `clean_text` to remove all control characters and replace repeating spaces with a single one; and `handle_chinese_chars`, which places spaces around Chinese characters. To replicate the `bert-base-uncased` tokenizer, we can just set this normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed57405-b4c3-4be1-8cfd-957deb1e03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc5d7d-e5b6-43c0-adbc-f34f783d238d",
   "metadata": {},
   "source": [
    "Generally speaking, however, when building a new tokenizer you wonâ€™t have access to such a handy normalizer already implemented in the ðŸ¤— Tokenizers library â€” so letâ€™s see how to create the BERT normalizer by hand. The library provides a `Lowercase` normalizer and a `StripAccents` normalizer, and you can compose several normalizers using a `Sequence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c292d6-290c-42b0-94fe-055f41b40990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(),\n",
    "     normalizers.Lowercase(),\n",
    "     normalizers.StripAccents()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6586eb-e5dc-41a9-a95c-e8b251648a75",
   "metadata": {},
   "source": [
    "Weâ€™re also using an `NFD` Unicode normalizer, as otherwise the `StripAccents` normalizer wonâ€™t properly recognize the accented characters and thus wonâ€™t strip them out.\n",
    "\n",
    "As weâ€™ve seen before, we can use the `normalize_str()` method of the `normalizer` to check out the effects it has on a given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfbc36af-b03d-4bf2-9a1f-b0f94fbe3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5bb3bf-713b-4815-b09b-2f8754f3ec5b",
   "metadata": {},
   "source": [
    "**To go further** If you test the two versions of the previous normalizers on a string containing the unicode character `u\"\\u0085\"` you will surely notice that these two normalizers are not exactly equivalent. To not over-complicate the version with `normalizers.Sequence` too much , we havenâ€™t included the Regex replacements that the `BertNormalizer` requires when the `clean_text` argument is set to `True` - which is the default behavior. But donâ€™t worry: it is possible to get exactly the same normalization without using the handy `BertNormalizer` by adding two `normalizers.Replace`â€™s to the normalizers sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94079e48-bc24-48d9-803a-6f8a0959b5a0",
   "metadata": {},
   "source": [
    "Next is the pre-tokenization step. Again, there is a prebuilt `BertPreTokenizer` that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad91b57-6221-461e-b317-ec7e5e6e1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51abdfc-74d4-4231-ba68-e84e95b04913",
   "metadata": {},
   "source": [
    "Or we can build it from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0150a20-b31f-4c4c-977d-6f17ac16534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254a849-d59e-45ac-8780-03e161b0dfad",
   "metadata": {},
   "source": [
    "Note that the `Whitespace` pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically splits on whitespace and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9795203c-6eb1-4840-af2f-bb6b73b3d5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c6dfc-d258-4a7b-b81b-708e33294e94",
   "metadata": {},
   "source": [
    "If you only want to split on whitespace, you should use the `WhitespaceSplit` pre-tokenizer instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34dc4821-7a79-4da6-ac27-1fa5e5c45f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf7d04-0e5b-455f-92a8-0849e2f44180",
   "metadata": {},
   "source": [
    "Like with normalizers, you can use a `Sequence` to compose several pre-tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e5a27fd-51a4-4917-aec7-2d1da488894e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [\n",
    "        pre_tokenizers.WhitespaceSplit(),\n",
    "        pre_tokenizers.Punctuation()\n",
    "    ]\n",
    ")\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca6cd8-3fb7-4d42-9ef4-e5ac377e0557",
   "metadata": {},
   "source": [
    "The next step in the tokenization pipeline is running the inputs through the model. We already specified our model in the initialization, but we still need to train it, which will require a `WordPieceTrainer`. The main thing to remember when instantiating a trainer in ðŸ¤— Tokenizers is that you need to pass it all the special tokens you intend to use â€” otherwise it wonâ€™t add them to the vocabulary, since they are not in the training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "295875d8-f17c-4915-93ff-54779ef26c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127e980-f8cd-41b7-9d2c-ca119e7d356c",
   "metadata": {},
   "source": [
    "As well as specifying the `vocab_size` and `special_tokens`, we can set the `min_frequency` (the number of times a token must appear to be included in the vocabulary) or change the `continuing_subword_prefix` (if we want to use something different from `##`).\n",
    "\n",
    "To train our model using the iterator we defined ealier, we just have to execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8bb7632-c283-455b-9f7b-9dcd2091812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc9d77-c227-4827-8f08-398a1620a5e5",
   "metadata": {},
   "source": [
    "We can also use text files to train our tokenizer, which would look like this (we reinitialize the model with an empty `WordPiece` beforehand):\n",
    "\n",
    "```python\n",
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06beae0f-f665-465a-a2db-4785990ce4fa",
   "metadata": {},
   "source": [
    "In both cases, we can then test the tokenizer on a text by calling the `encode()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69b0ff1-16d7-413b-bb2e-5bd0f3113c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082bda0-6390-4311-960a-c7301e741ee9",
   "metadata": {},
   "source": [
    "The `encoding` obtained is an `Encoding`, which contains all the necessary outputs of the tokenizer in its various attributes: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, and overflowing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27995d6-131f-4eab-8627-a540c36912f3",
   "metadata": {},
   "source": [
    "The last step in the tokenization pipeline is post-processing. We need to add the `[CLS]` token at the beginning and the `[SEP]` token at the end (or after each sentence, if we have a pair of sentences). We will use a `TemplateProcessor` for this, but first we need to know the IDs of the `[CLS]` and `[SEP]` tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d475d48-39fe-4998-bcbd-09138a746de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb0985-02cb-4f63-97b1-ab188a80e3ae",
   "metadata": {},
   "source": [
    "To write the template for the `TemplateProcessor`, we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon.\n",
    "\n",
    "The classic BERT template is thus defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1546eff-20b6-49ff-99a6-8ccc39d08a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9675e6d-c84f-411a-9081-7bbe112fb273",
   "metadata": {},
   "source": [
    "Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a2e2e-7086-4bc0-952e-e01681a647fb",
   "metadata": {},
   "source": [
    "Once this is added, going back to our previous example will give:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5410514-2f56-4b49-a7ed-5ca5ae644c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ce6dd-4d7d-470b-808b-2a35ca8497d9",
   "metadata": {},
   "source": [
    "And on a pair of sentences, we get the proper result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caca605d-8ef2-4d93-847f-87e8d95b5386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7956e16-0a77-4408-9625-2e2679225ff7",
   "metadata": {},
   "source": [
    "Weâ€™ve almost finished building this tokenizer from scratch â€” the last step is to include a decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474b0bc3-c318-4ebd-b217-5336530aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c88767-acb7-4ea0-b315-1a5749556cd1",
   "metadata": {},
   "source": [
    "Letâ€™s test it on our previous `encoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fddca9f-e9d6-465f-9763-2cc56d97f26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfce40c-e9b5-49b1-be72-742b4c4871cf",
   "metadata": {},
   "source": [
    "Great! We can save our tokenizer in a single JSON file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "826d60ab-e7ee-40b3-9a97-459597e80860",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb834bf9-2bea-4c59-8f63-c79ccea8e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat tokenizer.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ab44d-1ee2-4cf9-ac9b-dac0730353c7",
   "metadata": {},
   "source": [
    "To use this tokenizer in ðŸ¤— Transformers, we have to wrap it in a `PreTrainedTokenizerFast`. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, `BertTokenizerFast`). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.\n",
    "\n",
    "To wrap the tokenizer in a `PreTrainedTokenizerFast`, we can either pass the tokenizer we built as a `tokenizer_object` or pass the tokenizer file we saved as `tokenizer_file`. The key thing to remember is that we have to manually set all the special tokens, since that class canâ€™t infer from the `tokenizer` object which token is the mask token, the [CLS] token, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70b3f373-ecd6-41d5-9852-b261c22494a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73510056-dd8d-4d6a-ace8-6050b4df21b7",
   "metadata": {},
   "source": [
    "If you are using a specific tokenizer class (like `BertTokenizerFast`), you will only need to specify the special tokens that are different from the default ones (here, none):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c89790a-365d-486f-8ece-9676d5e9d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53206591-6149-4122-84ab-f55611c27ea0",
   "metadata": {},
   "source": [
    "You can then use this tokenizer like any other ðŸ¤— Transformers tokenizer. You can save it with the `save_pretrained()` method, or upload it to the Hub with the `push_to_hub()` method.\n",
    "\n",
    "Now that weâ€™ve seen how to build a WordPiece tokenizer, letâ€™s do the same for a BPE tokenizer. Weâ€™ll go a bit faster since you know all the steps, and only highlight the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8b83e-e899-40db-96e9-20fc44e34127",
   "metadata": {},
   "source": [
    "###  Building a BPE tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bbe97-9180-4da4-977d-e06d210e2129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
