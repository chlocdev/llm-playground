{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8094ed44-e45c-4f1a-aff3-5faac5e79798",
   "metadata": {},
   "source": [
    "###  Unigram tokenization\n",
    "\n",
    "The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.\n",
    "\n",
    "\n",
    "###  Training algorithm\n",
    "\n",
    "Compared to BPE and WordPiece, Unigram works in the other direction: it starts from a big vocabulary and removes tokens from it until it reaches the desired vocabulary size. There are several options to use to build that base vocabulary: we can take the most common substrings in pre-tokenized words, for instance, or apply BPE on the initial corpus with a large vocabulary size.\n",
    "\n",
    "At each step of the training, the Unigram algorithm computes a loss over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a lower effect on the overall loss over the corpus, so in a sense they are “less needed” and are the best candidates for removal.\n",
    "\n",
    "This is all a very costly operation, so we don’t just remove the single symbol associated with the lowest loss increase, but thepp ($\\(p\\$) being a hyperparameter you can control, usually 10 or 20) percent of the symbols associated with the lowest loss increase. This process is then repeated until the vocabulary has reached the desired size.\n",
    "\n",
    "Note that we never remove the base characters, to make sure any word can be tokenized.\n",
    "\n",
    "Now, this is still a bit vague: the main part of the algorithm is to compute a loss over the corpus and see how it changes when we remove some tokens from the vocabulary, but we haven’t explained how to do this yet. This step relies on the tokenization algorithm of a Unigram model, so we’ll dive into this next.\n",
    "\n",
    "We’ll reuse the corpus from the previous examples:\n",
    "\n",
    "```\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "and for this example, we will take all strict substrings for the initial vocabulary :\n",
    "\n",
    "```\n",
    "[\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"]\n",
    "```\n",
    "\n",
    "###  Tokenization algorithm\n",
    "\n",
    "A Unigram model is a type of language model that considers each token to be independent of the tokens before it. It’s the simplest language model, in the sense that the probability of token X given the previous context is just the probability of token X. So, if we used a Unigram language model to generate text, we would always predict the most common token.\n",
    "\n",
    "The probability of a given token is its frequency (the number of times we find it) in the original corpus, divided by the sum of all frequencies of all tokens in the vocabulary (to make sure the probabilities sum up to 1). For instance, \"ug\" is present in \"hug\", \"pug\", and \"hugs\", so it has a frequency of 20 in our corpus.\n",
    "\n",
    "Here are the frequencies of all the possible subwords in the vocabulary:\n",
    "\n",
    "```\n",
    "(\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
    "(\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
    "```\n",
    "\n",
    "So, the sum of all frequencies is 210, and the probability of the subword \"ug\" is thus 20/210.\n",
    "\n",
    "✏️ Now your turn! Write the code to compute the fr\n",
    "\n",
    "Now, to tokenize a given word, we look at all the possible segmentations into tokens and compute the probability of each according to the Unigram model. Since all tokens are considered independent, this probability is just the product of the probability of each token. For instance, the tokenization `[\"p\", \"u\", \"g\"]` of `\"pug\"` has the probability: \n",
    "\n",
    "$P(\\text{[``p\", ``u\", ``g\"]}) = P(\\text{``p\"}) \\times P(\\text{``u\"}) \\times P(\\text{``g\"}) = \\frac{5}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} = 0.000389\n",
    "$\n",
    "\n",
    "Comparatively, the tokenization `[\"pu\", \"g\"]` has the probability:\n",
    "\n",
    "$P(\\text{[``pu\", ``g\"]}) = P(\\text{``pu\"}) \\times P(\\text{``g\"}) = \\frac{5}{210} \\times \\frac{20}{210} = 0.0022676\n",
    "$\n",
    "\n",
    "so that one is way more likely. In general, tokenizations with the least tokens possible will have the highest probability (because of that division by 210 repeated for each token), which corresponds to what we want intuitively: to split a word into the least number of tokens possible.\n",
    "\n",
    "The tokenization of a word with the Unigram model is then the tokenization with the highest probability. In the example of \"pug\", here are the probabilities we would get for each possible segmentation:\n",
    "\n",
    "```\n",
    "[\"p\", \"u\", \"g\"] : 0.000389\n",
    "[\"p\", \"ug\"] : 0.0022676\n",
    "[\"pu\", \"g\"] : 0.0022676\n",
    "```\n",
    "\n",
    "So, \"pug\" would be tokenized as `[\"p\", \"ug\"]` or `[\"pu\", \"g\"]`, depending on which of those segmentations is encountered first (note that in a larger corpus, equality cases like this will be rare).\n",
    "\n",
    "In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it’s going to be a bit harder. There is a classic algorithm used for this, called the Viterbi algorithm. Essentially, we can build a graph to detect the possible segmentations of a given word by saying there is a branch from character a to character b if the subword from a to b is in the vocabulary, and attribute to that branch the probability of the subword.\n",
    "\n",
    "To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end.\n",
    "\n",
    "Let’s take a look at an example using our vocabulary and the word `\"unhug\"`. For each position, the subwords with the best scores ending there are the following:\n",
    "\n",
    "```\n",
    "Character 0 (u): \"u\" (score 0.171429)\n",
    "Character 1 (n): \"un\" (score 0.076191)\n",
    "Character 2 (h): \"un\" \"h\" (score 0.005442)\n",
    "Character 3 (u): \"un\" \"hu\" (score 0.005442)\n",
    "Character 4 (g): \"un\" \"hug\" (score 0.005442)\n",
    "```\n",
    "\n",
    "Thus \"unhug\" would be tokenized as `[\"un\", \"hug\"]`.\n",
    "\n",
    "✏️ Now your turn! Determine the tokenization of the word \"huggun\", and its score.\n",
    "\n",
    "###  Back to training\n",
    "\n",
    "Now that we have seen how the tokenization works, we can dive a little more deeply into the loss used during training. At any given stage, this loss is computed by tokenizing every word in the corpus, using the current vocabulary and the Unigram model determined by the frequencies of each token in the corpus (as seen before).\n",
    "\n",
    "Each word in the corpus has a score, and the loss is the negative log likelihood of those scores — that is, the sum for all the words in the corpus of all the `-log(P(word))`.\n",
    "\n",
    "Let’s go back to our example with the following corpus:\n",
    "\n",
    "```\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "The tokenization of each word with their respective scores is:\n",
    "\n",
    "```\n",
    "\"hug\": [\"hug\"] (score 0.071428)\n",
    "\"pug\": [\"pu\", \"g\"] (score 0.007710)\n",
    "\"pun\": [\"pu\", \"n\"] (score 0.006168)\n",
    "\"bun\": [\"bu\", \"n\"] (score 0.001451)\n",
    "\"hugs\": [\"hug\", \"s\"] (score 0.001701)\n",
    "```\n",
    "\n",
    "So the loss is:\n",
    "\n",
    "```\n",
    "10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8\n",
    "```\n",
    "\n",
    "Now we need to compute how removing each token affects the loss. This is rather tedious, so we’ll just do it for two tokens here and save the whole process for when we have code to help us. In this (very) particular case, we had two equivalent tokenizations of all the words: as we saw earlier, for example, `\"pug\"` could be tokenized `[\"p\", \"ug\"]` with the same score. Thus, removing the `\"pu\"` token from the vocabulary will give the exact same loss.\n",
    "\n",
    "On the other hand, removing `\"hug\"` will make the loss worse, because the tokenization of `\"hug\"` and `\"hugs\"` will become:\n",
    "\n",
    "```\n",
    "\"hug\": [\"hu\", \"g\"] (score 0.006802)\n",
    "\"hugs\": [\"hu\", \"gs\"] (score 0.001701)\n",
    "```\n",
    "\n",
    "These changes will cause the loss to rise by:\n",
    "\n",
    "```\n",
    "- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5\n",
    "```\n",
    "\n",
    "Therefore, the token `\"pu\"` will probably be removed from the vocabulary, but not `\"hug\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9c791-1e90-4b39-a48e-3ec06d467d86",
   "metadata": {},
   "source": [
    "###  Implementing Unigram\n",
    "\n",
    "Now let’s implement everything we’ve seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better.\n",
    "\n",
    "We will use the same corpus as before as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091b05b8-0b51-487a-9a1b-9d012a3e7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb1ed5-84ce-4eb3-8d03-fec58908e7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
