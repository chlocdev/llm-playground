{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb6dba7-534d-4c13-a3a9-3985630fea8a",
   "metadata": {},
   "source": [
    "- [Fsoft-AIC/XMAiNframe-instruct-7b](https://huggingface.co/Fsoft-AIC/XMAiNframe-instruct-7b)\n",
    "- [CohereForAI/aya-23-8B](https://huggingface.co/CohereForAI/aya-23-8B)\n",
    "- [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)\n",
    "- [Qwen/Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)\n",
    "- [Qwen/Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)\n",
    "- [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "- [facebook/m2m100_418M](https://huggingface.co/facebook/m2m100_418M)\n",
    "- [facebook/nllb-200-distilled-600M](facebook/nllb-200-distilled-600M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094f5f9-52f6-48a4-8074-aceef15dab5a",
   "metadata": {},
   "source": [
    "### Fsoft-AIC/XMAiNframe-instruct-7b\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM: 27GB\n",
    "\n",
    "-GPU RAM:\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b485bbd-af34-4946-b2b6-2f070854a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fsoft-AIC/XMAiNframe-instruct-7b \n",
      "\n",
      "Original[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "Tokenize[47]: ['å½¼', 'ãģ¯', 'æ¯', 'İ', 'æľĿ', '6', 'æĻ', 'Ĥ', 'ãģ«', 'èµ·', 'ãģ', 'į', 'ãģ¦', 'ãĢģ', 'ãĤ', '³', 'ãĥ¼ãĥ', 'Ĵ', 'ãĥ¼', 'ãĤĴ', 'é£', '²', 'ãĤ', 'ĵ', 'ãģ', 'ł', 'å¾', 'Į', 'ãĢģ', 'çĬ¬', 'ãģ¨', 'ä¸Ģ', 'ç', '·', 'Ĵ', 'ãģ«', 'æķ£', 'æŃ', '©', 'ãģ«', 'åĩº', 'ãģĭ', 'ãģ', 'ĳ', 'ãģ¾', 'ãģĻ', 'ãĢĤ'] \n",
      "\n",
      "Ids[47]: [19686, 60145, 971, 223, 8702, 21, 1528, 211, 45223, 1762, 7217, 222, 78373, 537, 9156, 111, 79366, 227, 31988, 43401, 1570, 110, 9156, 228, 7217, 241, 667, 221, 537, 30894, 58372, 505, 161, 115, 227, 45223, 9113, 888, 102, 45223, 1029, 89947, 7217, 226, 78590, 46891, 398] \n",
      "\n",
      "Text[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "The Same: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "repo_id = \"Fsoft-AIC/XMAiNframe-instruct-7b\"\n",
    "sentence = \"彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence) # Tokenize sentence\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Convert to ids\n",
    "text = tokenizer.decode(ids) # Convert ids back to text\n",
    "\n",
    "# Print out\n",
    "print(repo_id,\"\\n\")\n",
    "print(f\"Original[{len(sentence)}]:\",sentence,\"\\n\")\n",
    "print(f\"Tokenize[{len(tokens)}]:\",tokens,\"\\n\")\n",
    "print(f\"Ids[{len(ids)}]:\",ids,\"\\n\")\n",
    "print(f\"Text[{len(text)}]:\",text,\"\\n\")\n",
    "print(f\"The Same: {text==sentence}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9469bf64-8116-47db-a56f-bc88b29f1031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436dfb91a1904a9e8db6fb44772ff7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Mainframe is uncertain, as many organizations are gradually shifting their focus towards cloud computing and distributed systems. However, Mainframes will continue to play a vital role in critical applications and industries that require high levels of security, reliability, and performance.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Fsoft-AIC/XMAiNframe-instruct-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Fsoft-AIC/XMAiNframe-instruct-7b\")\n",
    "messages=[\n",
    "    {'from':'system', 'value': \"You are a helpful assistant\"},\n",
    "    {'from': 'human', 'value': 'What is the future of Mainframe?'}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    " \n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e414b0-4c53-4213-9296-733e130b92bc",
   "metadata": {},
   "source": [
    "### CohereForAI/aya-23-8B\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM: 31GB\n",
    "\n",
    "-GPU RAM:\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d1f20a-eec9-4dc6-97e8-4a80bda865ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CohereForAI/aya-23-8B \n",
      "\n",
      "Original[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "Tokenize[25]: ['å½¼ãģ¯', 'æ¯İ', 'æľĿ', '6', 'æĻĤãģ«', 'èµ·ãģį', 'ãģ¦', 'ãĢģ', 'ãĤ³', 'ãĥ¼ãĥĴ', 'ãĥ¼', 'ãĤĴ', 'é£²', 'ãĤĵãģł', 'å¾Į', 'ãĢģ', 'çĬ¬', 'ãģ¨', 'ä¸Ģç·Ĵãģ«', 'æķ£', 'æŃ©', 'ãģ«åĩº', 'ãģĭãģĳ', 'ãģ¾ãģĻ', 'ãĢĤ'] \n",
      "\n",
      "Ids[25]: [83600, 44967, 7283, 29, 55745, 195011, 2903, 2066, 6581, 162321, 2406, 2784, 62566, 33696, 4945, 2066, 45498, 2745, 154325, 21331, 42850, 69640, 61926, 73333, 1967] \n",
      "\n",
      "Text[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "The Same: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "repo_id = \"CohereForAI/aya-23-8B\"\n",
    "sentence = \"彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence) # Tokenize sentence\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Convert to ids\n",
    "text = tokenizer.decode(ids) # Convert ids back to text\n",
    "\n",
    "# Print out\n",
    "print(repo_id,\"\\n\")\n",
    "print(f\"Original[{len(sentence)}]:\",sentence,\"\\n\")\n",
    "print(f\"Tokenize[{len(tokens)}]:\",tokens,\"\\n\")\n",
    "print(f\"Ids[{len(ids)}]:\",ids,\"\\n\")\n",
    "print(f\"Text[{len(text)}]:\",text,\"\\n\")\n",
    "print(f\"The Same: {text==sentence}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5b9e94-fba8-4376-84b5-01f09bf0e004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a86cd5ce6c46f9a1961c330a985dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Sevgili Anne,\n",
      "\n",
      "Bugün sana, kalbimin en derinliklerinden gelen bir mektup yazmak istiyorum. Seni ne kadar çok sevdiğimi ve hayatımın her anında ne kadar özel bir yer tuttuğunu ifade etmek istiyorum.\n",
      "\n",
      "Seni tanıdığım andan itibaren, hayatımdaki her şey değişti. Sevgin ve desteğin, bana dünyayı fethetmek için gereken gücü verdi. Her zaman benim için oradasın\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"CohereForAI/aya-23-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,device_map=\"auto\")\n",
    "\n",
    "# Format message with the command-r-plus chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=100, \n",
    "    do_sample=True, \n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0])\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1307af-1c5a-4141-ad67-ab9839c9a2e8",
   "metadata": {},
   "source": [
    "### OrionStarAI/Orion-14B-Chat\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM:\n",
    "\n",
    "-GPU RAM: 30GB (blfoat16)\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6b454a-f46f-4133-b7fc-70ae8318f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrionStarAI/Orion-14B-Chat \n",
      "\n",
      "Original[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "Tokenize[21]: ['彼は', '毎', '朝', '6', '時に', '起', 'きて', '、', 'コーヒー', 'を飲', 'んだ', '後', '、', '犬', 'と一緒に', '散', '歩', 'に出', 'かけ', 'ます', '。'] \n",
      "\n",
      "Ids[21]: [80688, 58009, 51419, 50486, 77530, 50528, 78321, 50400, 82102, 81519, 77079, 50955, 50400, 53129, 80561, 51582, 58866, 77716, 77643, 76851, 50381] \n",
      "\n",
      "Text[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "The Same: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "repo_id = \"OrionStarAI/Orion-14B-Chat\"\n",
    "sentence = \"彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence) # Tokenize sentence\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Convert to ids\n",
    "text = tokenizer.decode(ids) # Convert ids back to text\n",
    "\n",
    "# Print out\n",
    "print(repo_id,\"\\n\")\n",
    "print(f\"Original[{len(sentence)}]:\",sentence,\"\\n\")\n",
    "print(f\"Tokenize[{len(tokens)}]:\",tokens,\"\\n\")\n",
    "print(f\"Ids[{len(ids)}]:\",ids,\"\\n\")\n",
    "print(f\"Text[{len(text)}]:\",text,\"\\n\")\n",
    "print(f\"The Same: {text==sentence}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baca1afc-768f-4ad4-af00-0d90af4f14bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447cc24c71ef475aa1680c9d70433719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI language model, so I don't have a name like a person does. You can refer to me as \"AI\" or \"chatbot.\" How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B-Chat\", device_map=\"auto\",\n",
    "                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B-Chat\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b95ad-f45d-426b-8fbc-043f4abae4ef",
   "metadata": {},
   "source": [
    "### Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM:\n",
    "\n",
    "-GPU RAM: 8GB (float32)\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ae68ee-3be3-4e1c-80a2-00365485c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-Coder-1.5B-Instruct \n",
      "\n",
      "Original[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "Tokenize[24]: ['å½¼', 'ãģ¯', 'æ¯', 'İ', 'æľĿ', '6', 'æĻĤãģ«', 'èµ·', 'ãģįãģ¦', 'ãĢģ', 'ãĤ³ãĥ¼ãĥĴãĥ¼', 'ãĤĴ', 'é£²', 'ãĤĵãģł', 'å¾Į', 'ãĢģ', 'çĬ¬', 'ãģ¨ä¸Ģç·Ĵãģ«', 'æķ£', 'æŃ©', 'ãģ«åĩº', 'ãģĭãģĳ', 'ãģ¾ãģĻ', 'ãĢĤ'] \n",
      "\n",
      "Ids[24]: [101429, 15322, 29975, 236, 99816, 21, 129868, 71618, 127918, 5373, 142260, 29412, 113515, 125486, 73382, 5373, 105329, 140469, 99632, 143852, 131296, 126713, 32441, 1773] \n",
      "\n",
      "Text[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "The Same: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "repo_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "sentence = \"彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence) # Tokenize sentence\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Convert to ids\n",
    "text = tokenizer.decode(ids) # Convert ids back to text\n",
    "\n",
    "# Print out\n",
    "print(repo_id,\"\\n\")\n",
    "print(f\"Original[{len(sentence)}]:\",sentence,\"\\n\")\n",
    "print(f\"Tokenize[{len(tokens)}]:\",tokens,\"\\n\")\n",
    "print(f\"Ids[{len(ids)}]:\",ids,\"\\n\")\n",
    "print(f\"Text[{len(text)}]:\",text,\"\\n\")\n",
    "print(f\"The Same: {text==sentence}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e292a72a-a38b-4ba4-ac9b-9e124c8bd41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is the implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# Example usage:\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "sorted_arr = quick_sort(arr)\n",
      "print(\"Sorted array:\", sorted_arr)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Base Case**: If the length of the array is less than or equal to 1, it means the array is already sorted, so we return it as is.\n",
      "2. **Select Pivot**: We choose the middle element of the array as the pivot. This can be any element, but the median is often used for better performance.\n",
      "3. **Partitioning**:\n",
      "   - `left`: Elements less than the pivot.\n",
      "   - `middle`: Elements equal to the pivot.\n",
      "   - `right`: Elements greater than the pivot.\n",
      "4. **Recursive Sorting**: We recursively apply the `quick_sort` function to the `left` and `right` sub-arrays and concatenate the results with the `middle` elements.\n",
      "\n",
      "This implementation ensures that the array is partitioned into two halves around the pivot, and then the process is repeated on each half until the entire array is sorted.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62eff53-22ad-470a-a6a8-edb1f36caff6",
   "metadata": {},
   "source": [
    "### Qwen/Qwen2.5-Coder-7B-Instruct\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM:\n",
    "\n",
    "-GPU RAM: 31GB (float32)\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b2801a-cf00-4e39-989d-71bd9c7e623e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793420b1453545febcd05d6a2b6dbdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40655e70d74c4ff99f0c0c88708be6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9104099bfab4d3aae635c2f74bd4f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-Coder-7B-Instruct \n",
      "\n",
      "Original[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "Tokenize[24]: ['å½¼', 'ãģ¯', 'æ¯', 'İ', 'æľĿ', '6', 'æĻĤãģ«', 'èµ·', 'ãģįãģ¦', 'ãĢģ', 'ãĤ³ãĥ¼ãĥĴãĥ¼', 'ãĤĴ', 'é£²', 'ãĤĵãģł', 'å¾Į', 'ãĢģ', 'çĬ¬', 'ãģ¨ä¸Ģç·Ĵãģ«', 'æķ£', 'æŃ©', 'ãģ«åĩº', 'ãģĭãģĳ', 'ãģ¾ãģĻ', 'ãĢĤ'] \n",
      "\n",
      "Ids[24]: [101429, 15322, 29975, 236, 99816, 21, 129868, 71618, 127918, 5373, 142260, 29412, 113515, 125486, 73382, 5373, 105329, 140469, 99632, 143852, 131296, 126713, 32441, 1773] \n",
      "\n",
      "Text[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "The Same: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "repo_id = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "sentence = \"彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence) # Tokenize sentence\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Convert to ids\n",
    "text = tokenizer.decode(ids) # Convert ids back to text\n",
    "\n",
    "# Print out\n",
    "print(repo_id,\"\\n\")\n",
    "print(f\"Original[{len(sentence)}]:\",sentence,\"\\n\")\n",
    "print(f\"Tokenize[{len(tokens)}]:\",tokens,\"\\n\")\n",
    "print(f\"Ids[{len(ids)}]:\",ids,\"\\n\")\n",
    "print(f\"Text[{len(text)}]:\",text,\"\\n\")\n",
    "print(f\"The Same: {text==sentence}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf512ba-cc33-4e2e-a2b5-0a11777a9690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7e7528f24c4853a317fd8efe1485dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! QuickSort is a popular and efficient sorting algorithm that uses the divide-and-conquer approach to sort elements. Below is a Python implementation of the QuickSort algorithm:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Example usage:\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "sorted_arr = quicksort(arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Base Case**: If the array has 0 or 1 elements, it is already sorted, so we return it as is.\n",
      "2. **Pivot Selection**: We choose the pivot element from the array. In this example, we select the middle element.\n",
      "3. **Partitioning**: We create three sub-arrays:\n",
      "   - `left`: All elements less than the pivot.\n",
      "   - `middle`: All elements equal to the pivot.\n",
      "   - `right`: All elements greater than the pivot.\n",
      "4. **Recursive Sorting**: We recursively apply the `quicksort` function to the `left` and `right` sub-arrays.\n",
      "5. **Combining Results**: Finally, we concatenate the sorted `left` sub-array, the `middle` sub-array, and the sorted `right` sub-array to get the final sorted array.\n",
      "\n",
      "This implementation is simple and easy to understand, but it may not be the most efficient in terms of space complexity due to the use of additional lists. For an in-place version of QuickSort, you can modify the code to sort the array within itself without using extra space for the sub-arrays.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb527a40-739d-4246-b535-9dcdd53e8970",
   "metadata": {},
   "source": [
    "### meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM:\n",
    "\n",
    "-GPU RAM: 31GB (float32)\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee66a823-c1de-4e7f-a5ba-b07c5ce5caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3.1-8B-Instruct \n",
      "\n",
      "Original[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "Tokenize[29]: ['å½¼', 'ãģ¯', 'æ¯İ', 'æľĿ', '6', 'æĻĤãģ«', 'èµ·', 'ãģį', 'ãģ¦', 'ãĢģ', 'ãĤ³', 'ãĥ¼ãĥ', 'Ĵ', 'ãĥ¼', 'ãĤĴ', 'é£²', 'ãĤĵãģł', 'å¾Į', 'ãĢģ', 'çĬ¬', 'ãģ¨', 'ä¸Ģç·Ĵ', 'ãģ«', 'æķ£', 'æŃ©', 'ãģ«åĩº', 'ãģĭãģĳ', 'ãģ¾ãģĻ', 'ãĢĤ'] \n",
      "\n",
      "Ids[29]: [103031, 15682, 121410, 103293, 21, 126101, 72718, 50834, 38144, 5486, 47260, 25005, 240, 11972, 30512, 115320, 102189, 74482, 5486, 123044, 19732, 126419, 20230, 107471, 105564, 117158, 126929, 33541, 1811] \n",
      "\n",
      "Text[35]: 彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。 \n",
      "\n",
      "The Same: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "sentence = \"彼は毎朝6時に起きて、コーヒーを飲んだ後、犬と一緒に散歩に出かけます。\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence) # Tokenize sentence\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Convert to ids\n",
    "text = tokenizer.decode(ids) # Convert ids back to text\n",
    "\n",
    "# Print out\n",
    "print(repo_id,\"\\n\")\n",
    "print(f\"Original[{len(sentence)}]:\",sentence,\"\\n\")\n",
    "print(f\"Tokenize[{len(tokens)}]:\",tokens,\"\\n\")\n",
    "print(f\"Ids[{len(ids)}]:\",ids,\"\\n\")\n",
    "print(f\"Text[{len(text)}]:\",text,\"\\n\")\n",
    "print(f\"The Same: {text==sentence}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dd1e34-4ac8-43a6-809c-8d1f0c82fe1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c91ef792164153bd0de9060850d5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrr, I be a swashbucklin' pirate chatbot, matey! Me name be Captain Chatbeard, and I be here to guide ye through the seven seas o' knowledge. I've got a treasure trove o' facts and answers at me disposal, and I be willin' to share 'em with ye, savvy?\"}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float32},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8182122-a100-4682-b8dc-8c3d314b9e48",
   "metadata": {},
   "source": [
    "### facebook/m2m100_418M\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM: 5GB\n",
    "\n",
    "-GPU RAM:\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4d40e8-9b5c-438d-a212-1ad2ddbe9f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Life is like a box of chocolate.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# translate Hindi to French\n",
    "tokenizer.src_lang = \"hi\"\n",
    "encoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# => \"La vie est comme une boîte de chocolat.\"\n",
    "\n",
    "# translate Chinese to English\n",
    "tokenizer.src_lang = \"zh\"\n",
    "encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# => \"Life is like a box of chocolate.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1cefb-8fcc-430c-ad3e-0f87d0316efc",
   "metadata": {},
   "source": [
    "### facebook/nllb-200-distilled-600M\n",
    "\n",
    "-Size:\n",
    "\n",
    "-CPU RAM: 4.5GB\n",
    "\n",
    "-GPU RAM:\n",
    "\n",
    "*Note: Restart notebook ro clear memory after inference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d75e46-7911-432a-81f9-f0de1df82077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Le chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "article = \"UN Chief says there is no military solution in Syria\"\n",
    "inputs = tokenizer(article, return_tensors=\"pt\")\n",
    "\n",
    "translated_tokens = model.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30\n",
    ")\n",
    "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5e2b98-83fb-4a1c-9564-eb95127c426c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Der UN-Chef sagt, es gibt keine militärische Lösung in Syrien.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\", token=True, src_lang=\"ron_Latn\"\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", token=True)\n",
    "\n",
    "article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n",
    "inputs = tokenizer(article, return_tensors=\"pt\")\n",
    "\n",
    "translated_tokens = model.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n",
    ")\n",
    "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
