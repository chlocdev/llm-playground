{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e08b76d-feaf-4557-a075-b5759503be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c08c881f514efda4e5fd082e670ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI language model created by OrionStar. My name is ChatGPT, but you can call me GPT for short. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B-Chat\", device_map=\"auto\",\n",
    "                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B-Chat\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174b9e4d-c76f-4cab-831a-c10c552e22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(sentence:str):\n",
    "    messages = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    return model.chat(tokenizer, messages, streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe13fdd1-f0b1-4695-949b-62d41ba87212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'こんにちは!あなたに会えて嬉しいです。何かお手伝いできますか？'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"こんにちは、お会いできて嬉しいです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e073d77f-dc23-47ad-a157-7312b9aaa6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess dataset\n",
    "QA_set = load_dataset(\"locchuong/Mainframe-QA-en-ja-500\")\n",
    "QA_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4a7861-3155-4a8a-a862-6691c93b066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " COBOLにおけるデータ分割の目的は?\n",
      "Answer:\n",
      " データ部門は、プログラムが使用するデータ項目(変数)およびレコーディングレイアウトを宣言するために使用されます。\n",
      "Completion:\n",
      " COBOLにおけるデータ分割の目的は、長いデータを小さなセクションに分割することです。これにより、データを処理しやすく、プログラムで操作しやすくなります。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "total_sample = QA_set['train'].num_rows\n",
    "\n",
    "random_idx = random.choice(range(total_sample))\n",
    "\n",
    "question = QA_set['train'][random_idx]['question_ja']\n",
    "\n",
    "answer = QA_set['train'][random_idx]['anwser_ja']\n",
    "\n",
    "completion = get_completion(question)\n",
    "\n",
    "print(\"Question:\\n\",question)\n",
    "\n",
    "print(\"Answer:\\n\",answer)\n",
    "\n",
    "print(\"Completion:\\n\",completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c4fda3-f1e4-4dfb-bb30-57f67fcd116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a451fb9b-09ad-4e1a-aa54-dcbfd073f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_completion(example):\n",
    "    return {\"Orion-14B-Chat-ja\":get_completion(example[\"question_ja\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0481ed0-09bf-424f-aa40-d98dab6e759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_subset = QA_set['train'].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172e5653-8945-4200-9c22-97bb32f08d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function add_completion at 0x7bb17f430700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6476a7e8186c413484b97fec94ad6988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja', 'Orion-14B-Chat-ja'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_subset = QA_subset.map(add_completion)\n",
    "\n",
    "QA_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96069eee-fd7b-40d8-b4d5-0b41a27bfa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f6a43a7b70436e82d7e4e68348f408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe65939138644e8a3102a7bb0d2f242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/Mainframe-QA-en-ja-200/commit/d257004434ba4e24925cbce1bd5a8156ebca3620', commit_message='Upload dataset', commit_description='', oid='d257004434ba4e24925cbce1bd5a8156ebca3620', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/Mainframe-QA-en-ja-200', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/Mainframe-QA-en-ja-200'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Mainframe-QA-en-ja-200\"\n",
    "QA_subset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687f0d1-c584-4fb2-9b1b-b8c176dd4d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
