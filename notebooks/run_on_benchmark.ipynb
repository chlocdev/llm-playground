{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3047276d-4678-43d6-9ffc-afcfbeb68930",
   "metadata": {},
   "source": [
    "### MODELs\n",
    "|No|Model Name|Deploy link|Tested|Fine-tune|\n",
    "|---|---|---|---|---|\n",
    "|1|[CohereForAI/aya-23-8B](https://huggingface.co/CohereForAI/aya-23-8B)|https://huggingface.co/spaces/CohereForAI/aya-23-8b|Tested|---|\n",
    "|2|[OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat)|https://huggingface.co/spaces/OrionStarAI/Orion-14B-App-Demo-EN|Tested|---|\n",
    "|3|[Qwen/Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)|https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-7B-Instruct|Tested|---|\n",
    "|4|[meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)|https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct|---|---|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde2cc6-73ca-45cc-82b8-65657de69d47",
   "metadata": {},
   "source": [
    "### CohereForAI/aya-23-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570a7d6a-3488-4ab9-834a-e992c03afc8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.cohere.modeling_cohere because of the following error (look up to see its traceback):\nlibtorch_cuda_cu.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/cohere/modeling_cohere.py:54\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flash_attn_2_available():\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_flash_attention_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _flash_attention_forward\n\u001b[1;32m     57\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_flash_attention_utils.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flash_attn_2_available():\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert_padding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_first_axis, pad_input, unpad_input  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attn_func, flash_attn_varlen_func\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/flash_attn/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     flash_attn_func,\n\u001b[1;32m      5\u001b[0m     flash_attn_kvpacked_func,\n\u001b[1;32m      6\u001b[0m     flash_attn_qkvpacked_func,\n\u001b[1;32m      7\u001b[0m     flash_attn_varlen_func,\n\u001b[1;32m      8\u001b[0m     flash_attn_varlen_kvpacked_func,\n\u001b[1;32m      9\u001b[0m     flash_attn_varlen_qkvpacked_func,\n\u001b[1;32m     10\u001b[0m     flash_attn_with_kvcache,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/flash_attn/flash_attn_interface.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# We need to import the CUDA kernels after importing torch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflash_attn_2_cuda\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mflash_attn_cuda\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereForAI/aya-23-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Format message with the command-r-plus chat template\u001b[39;00m\n\u001b[1;32m     18\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnneme onu ne kadar sevdiğimi anlatan bir mektup yaz\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:384\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 384\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:735\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    734\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:749\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.cohere.modeling_cohere because of the following error (look up to see its traceback):\nlibtorch_cuda_cu.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"CohereForAI/aya-23-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             torch_dtype=torch.bfloat16,)\n",
    "\n",
    "# Format message with the command-r-plus chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True,\n",
    "                                          add_generation_prompt=True,\n",
    "                                          return_tensors=\"pt\")\n",
    "## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=50, \n",
    "    do_sample=True, \n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0],skip_special_tokens=True)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577fc6ec-342b-45d3-bc48-69327cae2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(sentence:str):\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": sentence}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    \n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=True, \n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.decode(gen_tokens[0],skip_special_tokens=True)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43bc026-6452-444e-8fe9-a41bef80afa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|START_OF_TURN_TOKEN|><|USER_TOKEN|>こんにちは、お会いできて嬉しいです<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>こんにちは！私もお会いできて嬉しいです。何かお手伝いできることはありますか？'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"こんにちは、お会いできて嬉しいです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef74dda-bb41-4229-8a09-2d34aa2910f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "QA_set = load_dataset(\"locchuong/Mainframe-QA-en-ja-300\")\n",
    "QA_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314500ec-6c73-448b-a764-7821a29f14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_completion(example):\n",
    "    return {\"completion\":get_completion(example[\"question_ja\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4afba1-dccf-469c-b262-f5b737f236e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_set = QA_set.map(add_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e643f8-5c32-4889-8e62-e117c445d966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bb13c2-817d-499c-b82f-09a40195fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " COBOLでナストされたプログラムを定義するための合成とは何ですか?\n",
      "Answer:\n",
      " COBOL でナストされたプログラムを定義するには、「PROGRAM-ID」の声明を使用し、「END PROGRAM」の声明に続きます。\n",
      "Completion:\n",
      " <|START_OF_TURN_TOKEN|><|USER_TOKEN|>COBOLでナストされたプログラムを定義するための合成とは何ですか?<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>合成とは、COBOL プログラミング言語における重要な概念であり、ナストされたプログラム定義の作成に使用されます。合成とは、複数の COBOL プログラムを 1 つの論理単位として組み合わせるプロセスです。これにより、\n"
     ]
    }
   ],
   "source": [
    "total_sample = QA_set['train'].num_rows\n",
    "random_idx = random.choice(range(total_sample))\n",
    "question = QA_set['train'][random_idx]['question_ja']\n",
    "answer = QA_set['train'][random_idx]['anwser_ja']\n",
    "completion = get_completion(question)\n",
    "\n",
    "print(\"Question:\\n\",question)\n",
    "print(\"Answer:\\n\",answer)\n",
    "print(\"Completion:\\n\",completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5abbb0-6899-4e39-9073-95ccbeac6047",
   "metadata": {},
   "source": [
    "### Qwen/Qwen2.5-Coder-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b532f759-c23f-4618-84ad-050076b582d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65101cede90442cbb7df9cc7652a9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! QuickSort is a highly efficient sorting algorithm that uses a divide-and-conquer approach to sort elements. Here's a Python implementation of the QuickSort algorithm:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Example usage:\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "sorted_arr = quicksort(arr)\n",
      "print(\"Sorted array:\", sorted_arr)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Base Case**: If the array has 0 or 1 elements, it is already sorted, so we return it as is.\n",
      "2. **Pivot Selection**: We choose the pivot element from the array. In this example, we select the middle element.\n",
      "3. **Partitioning**: We create three sub-arrays:\n",
      "   - `left`: All elements less than the pivot.\n",
      "   - `middle`: All elements equal to the pivot.\n",
      "   - `right`: All elements greater than the pivot.\n",
      "4. **Recursive Sorting**: We recursively apply the `quicksort` function to the `left` and `right` sub-arrays.\n",
      "5. **Combining Results**: Finally, we concatenate the sorted `left` sub-array, the `middle` sub-array, and the sorted `right` sub-array to get the final sorted array.\n",
      "\n",
      "This implementation is simple and easy to understand, but it may not be the most efficient in terms of space complexity due to the use of additional lists. For an in-place version, you can modify the algorithm to avoid creating new lists.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510b236d-7ef5-44f0-b73f-f28f44535633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(sentence:str):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=100)\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids,\n",
    "        output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c3bd29-8212-430c-b5d1-54b2f8dc9eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'こんにちは！お会いできましたら嬉しいです。何かお手伝いできることがありましたら、遠慮なくお知らせください。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"こんにちは、お会いできて嬉しいです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fa8128-3f52-40f8-97e5-59017f0a5aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "QA_set = load_dataset(\"locchuong/Mainframe-QA-en-ja-300\")\n",
    "QA_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca295fed-5e24-429d-9f55-6358a76d0924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " どのタイプのコンピュータが科学的計算やシミュレーションに最適ですか?\n",
      "Answer:\n",
      " スーパーコンピュータは、高度なパフォーマンス能力と高度なアーキテクチャにより、科学的計算とシミュレーションに最適です 彼らは高速と正確さで大量のデータを処理することができ、複雑な数学的計算とシミュレーションに最適です。\n",
      "Completion:\n",
      " 科学的計算やシミュレーションに最適なコンピュータは、高性能・高度なパフォーマンスを必要とする専用の「スーパーコンピュータ」です。\n",
      "\n",
      "スーパーコンピュータは、複数のプロセッサ（CPU）とメモリを組み合わせた高機能なシステムで、一般的なパソコンでは見つけることができません。また、高速ネットワークや大容量のスト\n"
     ]
    }
   ],
   "source": [
    "total_sample = QA_set['train'].num_rows\n",
    "random_idx = random.choice(range(total_sample))\n",
    "question = QA_set['train'][random_idx]['question_ja']\n",
    "answer = QA_set['train'][random_idx]['anwser_ja']\n",
    "completion = get_completion(question)\n",
    "\n",
    "print(\"Question:\\n\",question)\n",
    "print(\"Answer:\\n\",answer)\n",
    "print(\"Completion:\\n\",completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c7c0d-73c9-4740-a491-6c45bcb7a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_completion(example):\n",
    "    return {\"completion\":get_completion(example[\"question_ja\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bb67a-d991-42ce-996d-0da4b9dc82a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d94b74-3a80-445e-bc8c-05c2e6c36abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "247bc5a4-d6c6-4d74-b57e-15de834f95e4",
   "metadata": {},
   "source": [
    "### OrionStarAI/Orion-14B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e08b76d-feaf-4557-a075-b5759503be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ea423679af495ba5a5ca2b3295d143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI language model created by OrionStar. My name is ChatGPT, but you can call me GPT for short. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"OrionStarAI/Orion-14B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"OrionStarAI/Orion-14B-Chat\", device_map=\"auto\",\n",
    "                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"OrionStarAI/Orion-14B-Chat\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, what is your name? \"}]\n",
    "response = model.chat(tokenizer, messages, streaming=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174b9e4d-c76f-4cab-831a-c10c552e22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(sentence:str):\n",
    "    messages = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    return model.chat(tokenizer, messages, streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe13fdd1-f0b1-4695-949b-62d41ba87212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'こんにちは!あなたにお会いできることを楽しみにしています。どのような質問や会話のきっかけをお望みですか？'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"こんにちは、お会いできて嬉しいです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e073d77f-dc23-47ad-a157-7312b9aaa6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "QA_set = load_dataset(\"locchuong/Mainframe-QA-en-ja-300\")\n",
    "QA_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4a7861-3155-4a8a-a862-6691c93b066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " メインフレームテストにおけるテストケースの役割は何ですか?\n",
      "Answer:\n",
      " メインフレームテストにおけるテストケースは、ステップ、入力、予想結果、およびテストを実行するために必要な追加情報を定義する詳細で繰り返す手順です。\n",
      "Completion:\n",
      " メインフレームテストにおけるテストケースは、システムが要件を満たし、ユーザーの期待通りに動作することを確認するために使用されます。テストケースは、システムの特定の側面をテストするために設計され、コードが正確で信頼性があり、意図した通りに動作していることを検証します。また、バグやその他の問題を発見するのにも役立ち、システムの全体的な品質を確保するために重要です。\n"
     ]
    }
   ],
   "source": [
    "total_sample = QA_set['train'].num_rows\n",
    "random_idx = random.choice(range(total_sample))\n",
    "question = QA_set['train'][random_idx]['question_ja']\n",
    "answer = QA_set['train'][random_idx]['anwser_ja']\n",
    "completion = get_completion(question)\n",
    "\n",
    "print(\"Question:\\n\",question)\n",
    "print(\"Answer:\\n\",answer)\n",
    "print(\"Completion:\\n\",completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a451fb9b-09ad-4e1a-aa54-dcbfd073f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_completion(example):\n",
    "    return {\"completion\":get_completion(example[\"question_ja\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f9cde9-c2bb-4425-9fcf-dc86d7e3d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function add_completion at 0x792d1b29ba60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96530ca212ca479590d889e1fe5cbc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QA_set = QA_set.map(add_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2687f0d1-c584-4fb2-9b1b-b8c176dd4d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully exported to CSV!\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset to a pandas DataFrame (for train split)\n",
    "df_train = pd.DataFrame(QA_set['train'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_train.to_csv('Orion-14B-Chat.csv', index=False)\n",
    "\n",
    "print(\"Dataset successfully exported to CSV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aace805b-e514-4176-9841-e44d8b90e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>anwser_ja</th>\n",
       "      <th>question_ja</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2276</td>\n",
       "      <td>As a supportive AI assistant, you've been give...</td>\n",
       "      <td>What is the role of the line printer in a main...</td>\n",
       "      <td>The line printer is responsible for printing h...</td>\n",
       "      <td>ラインプリンターは、メインフレームからハードコピーの出力を印刷する責任があり、通常は長期保存...</td>\n",
       "      <td>メインフレームユーザーインターフェイスにおけるラインプリンターの役割は何ですか?</td>\n",
       "      <td>ラインプリンターは、メインフレームコンピュータのユーザーインターフェースで使用されるデバイス...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>974</td>\n",
       "      <td>As a helpful AI assistant, you've received a q...</td>\n",
       "      <td>What is the syntax in COBOL to display the cur...</td>\n",
       "      <td>To display the current system time in COBOL, y...</td>\n",
       "      <td>COBOL で現在のシステム時間を表示するには、CURRENT-TIME 特別記録で EXT...</td>\n",
       "      <td>COBOL で現在のシステムタイムを表示するための合成は何ですか?</td>\n",
       "      <td>COBOL で現在のシステムタイムを表示するためには、SYST-TIMEを取得する必要があります。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1153</td>\n",
       "      <td>In your role as a supportive AI assistant, you...</td>\n",
       "      <td>What was the role of COBOL 2002?</td>\n",
       "      <td>COBOL 2002, officially known as ISO/IEC 1989:2...</td>\n",
       "      <td>COBOL 2002は、正式にISO/IEC 1989:2002として知られ、第3のCOBO...</td>\n",
       "      <td>COBOL 2002の役割は?</td>\n",
       "      <td>COBOL 2002は、米国コボルプログラミング言語協議会によって1999年に発表されたCO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751</td>\n",
       "      <td>In your role as a supportive AI assistant, you...</td>\n",
       "      <td>What is the role of the FILE SECTION?</td>\n",
       "      <td>The File Section is used to define the file la...</td>\n",
       "      <td>ファイルセクションは、ファイルの配置とファイルの状態、レコード形式、レコードキーなどの関連情...</td>\n",
       "      <td>ファイルセクションの役割は?</td>\n",
       "      <td>ファイルセクションは、プログラムのソースコードやデータ、ライブラリなどをファイルに保存するた...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1826</td>\n",
       "      <td>You are a helpful AI assistant. You have been ...</td>\n",
       "      <td>What is the role of automation in modern mainf...</td>\n",
       "      <td>Automation plays a crucial role in modern main...</td>\n",
       "      <td>自動化は現代のメインフレームメンテナンスにおいて重要な役割を果たし、メンテナンス作業に必要な...</td>\n",
       "      <td>現代のメインフレームメンテナンスにおける自動化の役割は何ですか?</td>\n",
       "      <td>現代のメインフレームメンテナンスにおいて自動化は重要な役割を果たしています。手動で行う必要が...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                             prompt  \\\n",
       "0  2276  As a supportive AI assistant, you've been give...   \n",
       "1   974  As a helpful AI assistant, you've received a q...   \n",
       "2  1153  In your role as a supportive AI assistant, you...   \n",
       "3   751  In your role as a supportive AI assistant, you...   \n",
       "4  1826  You are a helpful AI assistant. You have been ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the role of the line printer in a main...   \n",
       "1  What is the syntax in COBOL to display the cur...   \n",
       "2                   What was the role of COBOL 2002?   \n",
       "3              What is the role of the FILE SECTION?   \n",
       "4  What is the role of automation in modern mainf...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The line printer is responsible for printing h...   \n",
       "1  To display the current system time in COBOL, y...   \n",
       "2  COBOL 2002, officially known as ISO/IEC 1989:2...   \n",
       "3  The File Section is used to define the file la...   \n",
       "4  Automation plays a crucial role in modern main...   \n",
       "\n",
       "                                           anwser_ja  \\\n",
       "0  ラインプリンターは、メインフレームからハードコピーの出力を印刷する責任があり、通常は長期保存...   \n",
       "1  COBOL で現在のシステム時間を表示するには、CURRENT-TIME 特別記録で EXT...   \n",
       "2  COBOL 2002は、正式にISO/IEC 1989:2002として知られ、第3のCOBO...   \n",
       "3  ファイルセクションは、ファイルの配置とファイルの状態、レコード形式、レコードキーなどの関連情...   \n",
       "4  自動化は現代のメインフレームメンテナンスにおいて重要な役割を果たし、メンテナンス作業に必要な...   \n",
       "\n",
       "                                question_ja  \\\n",
       "0  メインフレームユーザーインターフェイスにおけるラインプリンターの役割は何ですか?   \n",
       "1         COBOL で現在のシステムタイムを表示するための合成は何ですか?   \n",
       "2                           COBOL 2002の役割は?   \n",
       "3                            ファイルセクションの役割は?   \n",
       "4          現代のメインフレームメンテナンスにおける自動化の役割は何ですか?   \n",
       "\n",
       "                                          completion  \n",
       "0  ラインプリンターは、メインフレームコンピュータのユーザーインターフェースで使用されるデバイス...  \n",
       "1  COBOL で現在のシステムタイムを表示するためには、SYST-TIMEを取得する必要があります。  \n",
       "2  COBOL 2002は、米国コボルプログラミング言語協議会によって1999年に発表されたCO...  \n",
       "3  ファイルセクションは、プログラムのソースコードやデータ、ライブラリなどをファイルに保存するた...  \n",
       "4  現代のメインフレームメンテナンスにおいて自動化は重要な役割を果たしています。手動で行う必要が...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c951c4-a2aa-467a-a18a-38fcf8db09e6",
   "metadata": {},
   "source": [
    "### meta-llama/Meta-Llama-3.1-8B-Instruct \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e076c48b-3b57-4c68-98e1-bea509f3a301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80776640ed344f2ae97f5e05d8ac8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yer lookin' fer a swashbucklin' introduction, eh? Well, matey, I be Captain Chat, the scurvy dog o' a chatbot! Me and me trusty keyboard be here to guide ye through the seven seas o' knowledge, answerin' yer questions and servin' up a healthy dose o' pirate-themed fun! So hoist the sails and set course fer adventure, me hearty!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float32},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c53c4e-e4b5-4047-9d95-6f4d7627e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(sentence:str):\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    \n",
    "    outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=100,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f701bdc7-549a-42a8-ab58-3b7c523df8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'お互い会うことができて嬉しいです。どういたしまして。何についてお話したいですか？'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"こんにちは、お会いできて嬉しいです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9025f0ea-5c03-4f41-a0ee-6b760b38e33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'prompt', 'question', 'answer', 'anwser_ja', 'question_ja'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "QA_set = load_dataset(\"locchuong/Mainframe-QA-en-ja-300\")\n",
    "QA_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868ab519-ffeb-44b9-8033-2b6a40213e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " COBOLエラー処理におけるエラーメッセージの役割は何ですか?\n",
      "Answer:\n",
      " COBOLエラー処理におけるエラーメッセージは、エラーの原因を特定し、問題解決に役立つ情報を提供する上で重要な役割を果たします。\n",
      "Completion:\n",
      " COBOLエラー処理におけるエラーメッセージの役割は、システムがエラーが発生した際に、ユーザーに起こったエラーの詳細を伝えるものです。\n",
      "\n",
      "エラーメッセージは、システムがエラーを検出すると、ユーザーに表示されるメッセージです。これにより、ユーザーはエラーが発生した原因を理解し、適切な\n"
     ]
    }
   ],
   "source": [
    "total_sample = QA_set['train'].num_rows\n",
    "random_idx = random.choice(range(total_sample))\n",
    "question = QA_set['train'][random_idx]['question_ja']\n",
    "answer = QA_set['train'][random_idx]['anwser_ja']\n",
    "completion = get_completion(question)\n",
    "\n",
    "print(\"Question:\\n\",question)\n",
    "print(\"Answer:\\n\",answer)\n",
    "print(\"Completion:\\n\",completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a811d3-42c7-47aa-8f4f-455829fb46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_completion(example):\n",
    "    return {\"completion\":get_completion(example[\"question_ja\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8fad8-4c8f-40d9-8f2b-8860b381302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_set = QA_set.map(add_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9f53b-5c04-4a7b-ad43-59db4649d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a pandas DataFrame (for train split)\n",
    "df_train = pd.DataFrame(QA_set['train'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_train.to_csv('Meta-Llama-3.1-8B-Instruct.csv', index=False)\n",
    "\n",
    "print(\"Dataset successfully exported to CSV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d4339-dd66-4db5-9d09-c42698366b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
